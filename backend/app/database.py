"""æ•°æ®åº“è¿æ¥å’Œä¼šè¯ç®¡ç† - PostgreSQL å¤šç”¨æˆ·æ•°æ®éš”ç¦»"""
import asyncio
from typing import Dict, Any
from datetime import datetime
from sqlalchemy import select, text
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession, async_sessionmaker
from sqlalchemy.orm import declarative_base
from fastapi import Request, HTTPException
from app.config import settings
from app.logger import get_logger

logger = get_logger(__name__)

# åˆ›å»ºåŸºç±»
Base = declarative_base()

# å¯¼å…¥æ‰€æœ‰æ¨¡å‹ï¼Œç¡®ä¿ Base.metadata èƒ½å¤Ÿå‘ç°å®ƒä»¬
# è¿™å¿…é¡»åœ¨ Base åˆ›å»ºä¹‹åã€init_db ä¹‹å‰å¯¼å…¥
from app.models import (
    Project, Outline, Character, Chapter, GenerationHistory,
    Settings, WritingStyle, ProjectDefaultStyle,
    RelationshipType, CharacterRelationship, Organization, OrganizationMember,
    StoryMemory, PlotAnalysis, AnalysisTask, BatchGenerationTask,
    RegenerationTask, Career, CharacterCareer, User, MCPPlugin, PromptTemplate
)

# å¼•æ“ç¼“å­˜ï¼šæ¯ä¸ªç”¨æˆ·ä¸€ä¸ªå¼•æ“
_engine_cache: Dict[str, Any] = {}

# é”ç®¡ç†ï¼šç”¨äºä¿æŠ¤å¼•æ“åˆ›å»ºè¿‡ç¨‹
_engine_locks: Dict[str, asyncio.Lock] = {}
_cache_lock = asyncio.Lock()

# ä¼šè¯ç»Ÿè®¡ï¼ˆç”¨äºç›‘æ§è¿æ¥æ³„æ¼ï¼‰
_session_stats = {
    "created": 0,
    "closed": 0,
    "active": 0,
    "errors": 0,
    "generator_exits": 0,
    "last_check": None
}


async def get_engine(user_id: str):
    """è·å–æˆ–åˆ›å»ºç”¨æˆ·ä¸“å±çš„æ•°æ®åº“å¼•æ“ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
    
    PostgreSQL: æ‰€æœ‰ç”¨æˆ·å…±äº«ä¸€ä¸ªæ•°æ®åº“ï¼Œé€šè¿‡user_idå­—æ®µéš”ç¦»æ•°æ®
    
    Args:
        user_id: ç”¨æˆ·ID
        
    Returns:
        ç”¨æˆ·ä¸“å±çš„å¼‚æ­¥å¼•æ“
    """
    # PostgreSQLæ¨¡å¼ï¼šæ‰€æœ‰ç”¨æˆ·å…±äº«åŒä¸€ä¸ªå¼•æ“
    cache_key = "shared_postgres"
    if cache_key in _engine_cache:
        return _engine_cache[cache_key]
    
    async with _cache_lock:
        if cache_key not in _engine_cache:
            # æ£€æµ‹æ•°æ®åº“ç±»å‹
            is_sqlite = 'sqlite' in settings.database_url.lower()
            
            # åŸºç¡€å¼•æ“å‚æ•°
            engine_args = {
                "echo": settings.database_echo_pool,
                "echo_pool": settings.database_echo_pool,
                "future": True,
            }
            
            if is_sqlite:
                # SQLite é…ç½®ï¼ˆä½¿ç”¨ NullPoolï¼Œä¸æ”¯æŒè¿æ¥æ± å‚æ•°ï¼‰
                engine_args["connect_args"] = {
                    "check_same_thread": False,
                    "timeout": 30.0,  # ç­‰å¾…é”é‡Šæ”¾çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
                }
                # å¯ç”¨è¿æ¥å‰æ£€æµ‹ä»¥æ”¯æŒæ›´å¥½çš„å¹¶å‘
                engine_args["pool_pre_ping"] = True
                
                logger.info("ğŸ“Š ä½¿ç”¨ SQLite æ•°æ®åº“ï¼ˆNullPoolï¼Œè¶…æ—¶30ç§’ï¼ŒWALæ¨¡å¼ï¼‰")
            else:
                # PostgreSQL é…ç½®ï¼ˆå®Œæ•´è¿æ¥æ± æ”¯æŒï¼‰
                connect_args = {
                    "server_settings": {
                        "application_name": settings.app_name,
                        "jit": "off",
                        "search_path": "public",
                    },
                    "command_timeout": 60,
                    "statement_cache_size": 500,
                }
                
                engine_args.update({
                    "pool_size": settings.database_pool_size,
                    "max_overflow": settings.database_max_overflow,
                    "pool_timeout": settings.database_pool_timeout,
                    "pool_pre_ping": settings.database_pool_pre_ping,
                    "pool_recycle": settings.database_pool_recycle,
                    "pool_use_lifo": settings.database_pool_use_lifo,
                    "pool_reset_on_return": settings.database_pool_reset_on_return,
                    "max_identifier_length": settings.database_max_identifier_length,
                    "connect_args": connect_args
                })
                
                total_connections = settings.database_pool_size + settings.database_max_overflow
                estimated_concurrent_users = total_connections * 2
                
                logger.info(
                    f"ğŸ“Š PostgreSQL è¿æ¥æ± é…ç½®:\n"
                    f"   â”œâ”€ æ ¸å¿ƒè¿æ¥: {settings.database_pool_size}\n"
                    f"   â”œâ”€ æº¢å‡ºè¿æ¥: {settings.database_max_overflow}\n"
                    f"   â”œâ”€ æ€»è¿æ¥æ•°: {total_connections}\n"
                    f"   â”œâ”€ è·å–è¶…æ—¶: {settings.database_pool_timeout}ç§’\n"
                    f"   â”œâ”€ è¿æ¥å›æ”¶: {settings.database_pool_recycle}ç§’\n"
                    f"   â””â”€ é¢„ä¼°å¹¶å‘: {estimated_concurrent_users}+ç”¨æˆ·"
                )
            
            engine = create_async_engine(settings.database_url, **engine_args)
            _engine_cache[cache_key] = engine
            
            # å¦‚æœæ˜¯ SQLiteï¼Œå¯ç”¨ WAL æ¨¡å¼ä»¥æ”¯æŒè¯»å†™å¹¶å‘
            if is_sqlite:
                try:
                    from sqlalchemy import event
                    from sqlalchemy.pool import NullPool
                    
                    @event.listens_for(engine.sync_engine, "connect")
                    def set_sqlite_pragma(dbapi_conn, connection_record):
                        cursor = dbapi_conn.cursor()
                        cursor.execute("PRAGMA journal_mode=WAL")
                        cursor.execute("PRAGMA synchronous=NORMAL")
                        cursor.execute("PRAGMA cache_size=-64000")  # 64MB ç¼“å­˜
                        cursor.execute("PRAGMA busy_timeout=30000")  # 30ç§’è¶…æ—¶
                        cursor.close()
                    
                    logger.info("âœ… SQLite WAL æ¨¡å¼å·²å¯ç”¨ï¼ˆæ”¯æŒè¯»å†™å¹¶å‘ï¼‰")
                except Exception as e:
                    logger.warning(f"âš ï¸ å¯ç”¨ WAL æ¨¡å¼å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        
        return _engine_cache[cache_key]


async def get_db(request: Request):
    """è·å–æ•°æ®åº“ä¼šè¯çš„ä¾èµ–å‡½æ•°
    
    ä» request.state.user_id è·å–ç”¨æˆ·IDï¼Œç„¶åè¿”å›è¯¥ç”¨æˆ·çš„æ•°æ®åº“ä¼šè¯
    """
    user_id = getattr(request.state, "user_id", None)
    
    if not user_id:
        raise HTTPException(status_code=401, detail="æœªç™»å½•æˆ–ç”¨æˆ·IDç¼ºå¤±")
    
    engine = await get_engine(user_id)
    
    AsyncSessionLocal = async_sessionmaker(
        engine,
        class_=AsyncSession,
        expire_on_commit=False
    )
    
    session = AsyncSessionLocal()
    session_id = id(session)
    
    global _session_stats
    _session_stats["created"] += 1
    _session_stats["active"] += 1
    
    # logger.debug(f"ğŸ“Š ä¼šè¯åˆ›å»º [User:{user_id}][ID:{session_id}] - æ´»è·ƒ:{_session_stats['active']}, æ€»åˆ›å»º:{_session_stats['created']}, æ€»å…³é—­:{_session_stats['closed']}")
    
    try:
        yield session
        if session.in_transaction():
            await session.rollback()
    except GeneratorExit:
        _session_stats["generator_exits"] += 1
        logger.warning(f"âš ï¸ GeneratorExit [User:{user_id}][ID:{session_id}] - SSEè¿æ¥æ–­å¼€ï¼ˆæ€»è®¡:{_session_stats['generator_exits']}æ¬¡ï¼‰")
        try:
            if session.in_transaction():
                await session.rollback()
                logger.info(f"âœ… äº‹åŠ¡å·²å›æ»š [User:{user_id}][ID:{session_id}]ï¼ˆGeneratorExitï¼‰")
        except Exception as rollback_error:
            _session_stats["errors"] += 1
            logger.error(f"âŒ GeneratorExitå›æ»šå¤±è´¥ [User:{user_id}][ID:{session_id}]: {str(rollback_error)}")
    except Exception as e:
        _session_stats["errors"] += 1
        logger.error(f"âŒ ä¼šè¯å¼‚å¸¸ [User:{user_id}][ID:{session_id}]: {str(e)}")
        try:
            if session.in_transaction():
                await session.rollback()
                logger.info(f"âœ… äº‹åŠ¡å·²å›æ»š [User:{user_id}][ID:{session_id}]ï¼ˆå¼‚å¸¸ï¼‰")
        except Exception as rollback_error:
            logger.error(f"âŒ å¼‚å¸¸å›æ»šå¤±è´¥ [User:{user_id}][ID:{session_id}]: {str(rollback_error)}")
        raise
    finally:
        try:
            if session.in_transaction():
                await session.rollback()
                logger.warning(f"âš ï¸ finallyä¸­å‘ç°æœªæäº¤äº‹åŠ¡ [User:{user_id}][ID:{session_id}]ï¼Œå·²å›æ»š")
            
            await session.close()
            
            _session_stats["closed"] += 1
            _session_stats["active"] -= 1
            _session_stats["last_check"] = datetime.now().isoformat()
            
            logger.debug(f"ğŸ“Š ä¼šè¯å…³é—­ [User:{user_id}][ID:{session_id}] - æ´»è·ƒ:{_session_stats['active']}, æ€»åˆ›å»º:{_session_stats['created']}, æ€»å…³é—­:{_session_stats['closed']}, é”™è¯¯:{_session_stats['errors']}")
            
            # ä½¿ç”¨ä¼˜åŒ–åçš„ä¼šè¯ç›‘æ§é˜ˆå€¼
            if _session_stats["active"] > settings.database_session_leak_threshold:
                logger.error(f"ğŸš¨ ä¸¥é‡å‘Šè­¦ï¼šæ´»è·ƒä¼šè¯æ•° {_session_stats['active']} è¶…è¿‡æ³„æ¼é˜ˆå€¼ {settings.database_session_leak_threshold}ï¼")
            elif _session_stats["active"] > settings.database_session_max_active:
                logger.warning(f"âš ï¸ è­¦å‘Šï¼šæ´»è·ƒä¼šè¯æ•° {_session_stats['active']} è¶…è¿‡è­¦å‘Šé˜ˆå€¼ {settings.database_session_max_active}ï¼Œå¯èƒ½å­˜åœ¨è¿æ¥æ³„æ¼ï¼")
            elif _session_stats["active"] < 0:
                logger.error(f"ğŸš¨ æ´»è·ƒä¼šè¯æ•°å¼‚å¸¸: {_session_stats['active']}ï¼Œç»Ÿè®¡å¯èƒ½ä¸å‡†ç¡®ï¼")
                
        except Exception as e:
            _session_stats["errors"] += 1
            logger.error(f"âŒ å…³é—­ä¼šè¯æ—¶å‡ºé”™ [User:{user_id}][ID:{session_id}]: {str(e)}", exc_info=True)
            try:
                await session.close()
            except:
                pass

async def init_db(user_id: str = None):
    """
    åˆå§‹åŒ–æ•°æ®åº“ï¼ˆå·²å¼ƒç”¨ï¼‰
    
    âš ï¸ æ­¤å‡½æ•°å·²å¼ƒç”¨ï¼Œä»…ä¿ç•™ç”¨äºå‘åå…¼å®¹
    
    æ–°çš„æœ€ä½³å®è·µ:
    - è¡¨ç»“æ„ç®¡ç†: ä½¿ç”¨ 'alembic upgrade head'
    - ç”¨æˆ·é…ç½®: Settings åœ¨é¦–æ¬¡è®¿é—®æ—¶è‡ªåŠ¨åˆ›å»ºï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
    
    Args:
        user_id: ç”¨æˆ·ID (å·²ä¸å†ä½¿ç”¨)
    """
    logger.warning(
        "âš ï¸ init_db() å·²å¼ƒç”¨ä¸”æ— å®é™…ä½œç”¨ï¼\n"
        "   - è¡¨ç»“æ„: ç”± Alembic ç®¡ç†\n"
        "   - ç”¨æˆ·é…ç½®: Settings API è‡ªåŠ¨åˆ›å»º\n"
        "   å»ºè®®ç§»é™¤æ­¤è°ƒç”¨"
    )


async def close_db():
    """å…³é—­æ‰€æœ‰æ•°æ®åº“è¿æ¥"""
    try:
        logger.info("æ­£åœ¨å…³é—­æ‰€æœ‰æ•°æ®åº“è¿æ¥...")
        for user_id, engine in _engine_cache.items():
            await engine.dispose()
            logger.info(f"ç”¨æˆ· {user_id} çš„æ•°æ®åº“è¿æ¥å·²å…³é—­")
        _engine_cache.clear()
        logger.info("æ‰€æœ‰æ•°æ®åº“è¿æ¥å·²å…³é—­")
    except Exception as e:
        logger.error(f"å…³é—­æ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}", exc_info=True)
        raise

async def get_database_stats():
    """è·å–æ•°æ®åº“è¿æ¥å’Œä¼šè¯ç»Ÿè®¡ä¿¡æ¯
    
    Returns:
        dict: åŒ…å«æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
    """
    from app.config import settings
    
    # è·å–è¿æ¥æ± è¯¦ç»†çŠ¶æ€
    pool_stats = {}
    cache_key = "shared_postgres"
    if cache_key in _engine_cache:
        engine = _engine_cache[cache_key]
        try:
            pool = engine.pool
            pool_stats = {
                "size": pool.size(),  # å½“å‰è¿æ¥æ± å¤§å°
                "checked_in": pool.checkedin(),  # å¯ç”¨è¿æ¥æ•°
                "checked_out": pool.checkedout(),  # æ­£åœ¨ä½¿ç”¨çš„è¿æ¥æ•°
                "overflow": pool.overflow(),  # æº¢å‡ºè¿æ¥æ•°
                "usage_percent": (pool.checkedout() / (settings.database_pool_size + settings.database_max_overflow)) * 100,
            }
        except Exception as e:
            logger.warning(f"è·å–è¿æ¥æ± çŠ¶æ€å¤±è´¥: {e}")
            pool_stats = {"error": str(e)}
    
    stats = {
        "session_stats": {
            "created": _session_stats["created"],
            "closed": _session_stats["closed"],
            "active": _session_stats["active"],
            "errors": _session_stats["errors"],
            "generator_exits": _session_stats["generator_exits"],
            "last_check": _session_stats["last_check"],
        },
        "pool_stats": pool_stats,  # æ–°å¢ï¼šè¿æ¥æ± å®æ—¶çŠ¶æ€
        "engine_cache": {
            "total_engines": len(_engine_cache),
            "engine_keys": list(_engine_cache.keys()),
        },
        "config": {
            "database_type": "PostgreSQL",
            "pool_size": settings.database_pool_size,
            "max_overflow": settings.database_max_overflow,
            "total_connections": settings.database_pool_size + settings.database_max_overflow,
            "pool_timeout": settings.database_pool_timeout,
            "pool_recycle": settings.database_pool_recycle,
            "session_max_active_threshold": settings.database_session_max_active,
            "session_leak_threshold": settings.database_session_leak_threshold,
        },
        "health": {
            "status": "healthy",
            "warnings": [],
            "errors": [],
        }
    }
    
    # å¥åº·æ£€æŸ¥
    if _session_stats["active"] > settings.database_session_leak_threshold:
        stats["health"]["status"] = "critical"
        stats["health"]["errors"].append(
            f"æ´»è·ƒä¼šè¯æ•° {_session_stats['active']} è¶…è¿‡æ³„æ¼é˜ˆå€¼ {settings.database_session_leak_threshold}"
        )
    elif _session_stats["active"] > settings.database_session_max_active:
        stats["health"]["status"] = "warning"
        stats["health"]["warnings"].append(
            f"æ´»è·ƒä¼šè¯æ•° {_session_stats['active']} è¶…è¿‡è­¦å‘Šé˜ˆå€¼ {settings.database_session_max_active}"
        )
    
    if _session_stats["active"] < 0:
        stats["health"]["status"] = "error"
        stats["health"]["errors"].append(f"æ´»è·ƒä¼šè¯æ•°å¼‚å¸¸: {_session_stats['active']}")
    
    # è¿æ¥æ± ä½¿ç”¨ç‡æ£€æŸ¥
    if pool_stats and "usage_percent" in pool_stats:
        usage = pool_stats["usage_percent"]
        if usage > 90:
            stats["health"]["status"] = "warning"
            stats["health"]["warnings"].append(f"è¿æ¥æ± ä½¿ç”¨ç‡è¿‡é«˜: {usage:.1f}%")
        elif usage > 95:
            stats["health"]["status"] = "critical"
            stats["health"]["errors"].append(f"è¿æ¥æ± å‡ ä¹è€—å°½: {usage:.1f}%")
    
    error_rate = (_session_stats["errors"] / max(_session_stats["created"], 1)) * 100
    if error_rate > 5:
        if stats["health"]["status"] == "healthy":
            stats["health"]["status"] = "warning"
        stats["health"]["warnings"].append(f"ä¼šè¯é”™è¯¯ç‡è¿‡é«˜: {error_rate:.2f}%")
    
    stats["health"]["error_rate"] = f"{error_rate:.2f}%"
    
    return stats


async def check_database_health(user_id: str = None) -> dict:
    """æ£€æŸ¥æ•°æ®åº“è¿æ¥å¥åº·çŠ¶æ€
    
    Args:
        user_id: å¯é€‰çš„ç”¨æˆ·IDï¼Œå¦‚æœæä¾›åˆ™æ£€æŸ¥ç‰¹å®šç”¨æˆ·çš„æ•°æ®åº“
        
    Returns:
        dict: å¥åº·æ£€æŸ¥ç»“æœ
    """
    result = {
        "healthy": True,
        "checks": {},
        "timestamp": datetime.now().isoformat()
    }
    
    try:
        # æ£€æŸ¥å¼•æ“æ˜¯å¦å­˜åœ¨
        cache_key = "shared_postgres"
        if user_id:
            engine = await get_engine(user_id)
        else:
            if cache_key not in _engine_cache:
                result["checks"]["engine"] = {"status": "not_initialized", "healthy": True}
                return result
            engine = _engine_cache[cache_key]
        
        # æµ‹è¯•æ•°æ®åº“è¿æ¥
        AsyncSessionLocal = async_sessionmaker(
            engine,
            class_=AsyncSession,
            expire_on_commit=False
        )
        
        async with AsyncSessionLocal() as session:
            # æ‰§è¡Œç®€å•æŸ¥è¯¢æµ‹è¯•è¿æ¥
            await session.execute(text("SELECT 1"))
            result["checks"]["connection"] = {"status": "ok", "healthy": True}
            
        # æ£€æŸ¥è¿æ¥æ± çŠ¶æ€ï¼ˆä»…PostgreSQLï¼‰
        if hasattr(engine.pool, 'size'):
            pool_status = {
                "size": engine.pool.size(),
                "checked_in": engine.pool.checkedin(),
                "checked_out": engine.pool.checkedout(),
                "overflow": engine.pool.overflow(),
                "healthy": True
            }
            
            # è¿æ¥æ± å¥åº·æ£€æŸ¥
            if engine.pool.overflow() >= settings.database_max_overflow:
                pool_status["healthy"] = False
                pool_status["warning"] = "è¿æ¥æ± æº¢å‡ºå·²æ»¡"
                result["healthy"] = False
            
            result["checks"]["pool"] = pool_status
        
    except Exception as e:
        result["healthy"] = False
        result["checks"]["error"] = {
            "status": "error",
            "message": str(e),
            "healthy": False
        }
        logger.error(f"æ•°æ®åº“å¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}", exc_info=True)
    
    return result


async def reset_session_stats():
    """é‡ç½®ä¼šè¯ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºæµ‹è¯•æˆ–ç»´æŠ¤ï¼‰"""
    global _session_stats
    _session_stats = {
        "created": 0,
        "closed": 0,
        "active": 0,
        "errors": 0,
        "generator_exits": 0,
        "last_check": datetime.now().isoformat()
    }
    logger.info("âœ… ä¼šè¯ç»Ÿè®¡ä¿¡æ¯å·²é‡ç½®")
    return _session_stats