"""Gemini Provider"""
from typing import Any, AsyncGenerator, Dict, List, Optional

from app.logger import get_logger
from app.services.ai_clients.gemini_client import GeminiClient
from .base_provider import BaseAIProvider

logger = get_logger(__name__)


class GeminiProvider(BaseAIProvider):
    def __init__(self, client: GeminiClient):
        self.client = client

    async def generate(
        self,
        prompt: str,
        model: str,
        temperature: float,
        max_tokens: int,
        system_prompt: Optional[str] = None,
        tools: Optional[List[Dict]] = None,
        tool_choice: Optional[str] = None,
    ) -> Dict[str, Any]:
        messages = [{"role": "user", "content": prompt}]
        return await self.client.chat_completion(
            messages=messages,
            model=model,
            temperature=temperature,
            max_tokens=max_tokens,
            system_prompt=system_prompt,
            tools=tools,
            tool_choice=tool_choice,
        )

    async def generate_stream(
        self,
        prompt: str,
        model: str,
        temperature: float,
        max_tokens: int,
        system_prompt: Optional[str] = None,
        tools: Optional[List[Dict]] = None,
        tool_choice: Optional[str] = None,
        user_id: Optional[str] = None,
    ) -> AsyncGenerator[str, None]:
        # å¦‚æœæœ‰å·¥å…·ï¼Œä½¿ç”¨çœŸæ­£çš„æµå¼å·¥å…·è°ƒç”¨
        if tools:
            logger.debug(f"ğŸ”§ GeminiProvider: æœ‰ {len(tools)} ä¸ªå·¥å…·ï¼Œä½¿ç”¨æµå¼å¤„ç†")
            messages = [{"role": "user", "content": prompt}]
            actual_tool_choice = tool_choice if tool_choice else "auto"
            
            tool_calls_buffer = []
            
            async for chunk in self.client.chat_completion_stream(
                messages=messages,
                model=model,
                temperature=temperature,
                max_tokens=max_tokens,
                system_prompt=system_prompt,
                tools=tools,
                tool_choice=actual_tool_choice,
            ):
                # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
                if chunk.get("tool_calls"):
                    tool_calls_buffer.extend(chunk["tool_calls"])
                    logger.debug(f"ğŸ”§ æ”¶åˆ°å·¥å…·è°ƒç”¨: {len(chunk['tool_calls'])} ä¸ª")
                
                # æ£€æŸ¥æ˜¯å¦ç»“æŸ
                if chunk.get("done"):
                    if tool_calls_buffer:
                        logger.info(f"ğŸ”§ æµå¼ç»“æŸï¼Œå¤„ç† {len(tool_calls_buffer)} ä¸ªå·¥å…·è°ƒç”¨")
                        from app.mcp import mcp_client
                        actual_user_id = user_id or ""
                        tool_results = await mcp_client.batch_call_tools(
                            user_id=actual_user_id,
                            tool_calls=tool_calls_buffer
                        )
                        # å°†å·¥å…·ç»“æœæ³¨å…¥åˆ°ä¸Šä¸‹æ–‡ä¸­
                        tool_context = mcp_client.build_tool_context(tool_results, format="markdown")
                        
                        # æ„å»ºæœ€ç»ˆæç¤ºè¯ï¼Œè¦æ±‚AIåŸºäºå·¥å…·ç»“æœå›ç­”
                        final_prompt = f"{prompt}\n\n{tool_context}\n\nè¯·åŸºäºä»¥ä¸Šå·¥å…·æŸ¥è¯¢ç»“æœï¼Œç»™å‡ºå®Œæ•´è¯¦ç»†çš„å›ç­”ã€‚"
                        final_messages = [{"role": "user", "content": final_prompt}]
                        
                        # é€’å½’è°ƒç”¨ç”Ÿæˆæœ€ç»ˆç»“æœ
                        async for final_chunk in self._generate_with_tools(
                            final_messages, model, temperature, max_tokens, system_prompt, tools, user_id
                        ):
                            yield final_chunk
                    break
                
                # è¾“å‡ºæ–‡æœ¬å†…å®¹
                if chunk.get("content"):
                    yield chunk["content"]
            return
        
        # æ— å·¥å…·æ—¶æ™®é€šæµå¼ç”Ÿæˆ
        messages = [{"role": "user", "content": prompt}]
        async for chunk in self.client.chat_completion_stream(
            messages=messages,
            model=model,
            temperature=temperature,
            max_tokens=max_tokens,
            system_prompt=system_prompt,
        ):
            # ç¡®ä¿åª yield å­—ç¬¦ä¸²å†…å®¹ï¼Œé¿å… yield å­—å…¸å¯¼è‡´ç±»å‹é”™è¯¯
            if isinstance(chunk, dict):
                if chunk.get("content"):
                    yield chunk["content"]
            else:
                yield chunk

    async def _generate_with_tools(
        self,
        messages: list,
        model: str,
        temperature: float,
        max_tokens: int,
        system_prompt: Optional[str] = None,
        tools: list = None,
        user_id: Optional[str] = None,
    ) -> AsyncGenerator[str, None]:
        """è¾…åŠ©æ–¹æ³•ï¼šå¸¦å·¥å…·çš„æµå¼ç”Ÿæˆ"""
        tool_calls_buffer = []
        
        async for chunk in self.client.chat_completion_stream(
            messages=messages,
            model=model,
            temperature=temperature,
            max_tokens=max_tokens,
            system_prompt=system_prompt,
            tools=tools,
            tool_choice="auto",
        ):
            if chunk.get("tool_calls"):
                tool_calls_buffer.extend(chunk["tool_calls"])
                logger.debug(f"ğŸ”§ _generate_with_tools æ”¶åˆ°å·¥å…·è°ƒç”¨: {len(chunk['tool_calls'])} ä¸ª")
            
            if chunk.get("done"):
                if tool_calls_buffer:
                    from app.mcp import mcp_client
                    actual_user_id = user_id or ""
                    tool_results = await mcp_client.batch_call_tools(
                        user_id=actual_user_id,
                        tool_calls=tool_calls_buffer
                    )
                    tool_context = mcp_client.build_tool_context(tool_results, format="markdown")
                    
                    messages.append({"role": "user", "content": f"{tool_context}\n\nè¯·åŸºäºä»¥ä¸Šå·¥å…·æŸ¥è¯¢ç»“æœï¼Œç»™å‡ºå®Œæ•´è¯¦ç»†çš„å›ç­”ã€‚"})
                    
                    async for final_chunk in self._generate_with_tools(
                        messages, model, temperature, max_tokens, system_prompt, tools, user_id
                    ):
                        yield final_chunk
                break
            
            if chunk.get("content"):
                yield chunk["content"]