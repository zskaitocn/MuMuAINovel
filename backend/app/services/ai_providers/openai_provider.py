"""OpenAI Provider"""
from typing import Any, AsyncGenerator, Dict, List, Optional

from app.logger import get_logger
from app.services.ai_clients.openai_client import OpenAIClient
from .base_provider import BaseAIProvider

logger = get_logger(__name__)


class OpenAIProvider(BaseAIProvider):
    """OpenAI æä¾›å•†"""

    def __init__(self, client: OpenAIClient):
        self.client = client

    async def generate(
        self,
        prompt: str,
        model: str,
        temperature: float,
        max_tokens: int,
        system_prompt: Optional[str] = None,
        tools: Optional[List[Dict]] = None,
        tool_choice: Optional[str] = None,
    ) -> Dict[str, Any]:
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        return await self.client.chat_completion(
            messages=messages,
            model=model,
            temperature=temperature,
            max_tokens=max_tokens,
            tools=tools,
            tool_choice=tool_choice,
        )

    async def generate_stream(
        self,
        prompt: str,
        model: str,
        temperature: float,
        max_tokens: int,
        system_prompt: Optional[str] = None,
        tools: Optional[List[Dict]] = None,
        tool_choice: Optional[str] = None,
        user_id: Optional[str] = None,
    ) -> AsyncGenerator[str, None]:
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        # å¦‚æœæœ‰å·¥å…·ï¼Œä½¿ç”¨çœŸæ­£çš„æµå¼å·¥å…·è°ƒç”¨
        if tools:
            logger.debug(f"ğŸ”§ OpenAIProvider: æœ‰ {len(tools)} ä¸ªå·¥å…·ï¼Œä½¿ç”¨æµå¼å¤„ç†")
            actual_tool_choice = tool_choice if tool_choice else "auto"
            
            tool_calls_buffer = []
            
            async for chunk in self.client.chat_completion_stream(
                messages=messages,
                model=model,
                temperature=temperature,
                max_tokens=max_tokens,
                tools=tools,
                tool_choice=actual_tool_choice,
            ):
                # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
                if chunk.get("tool_calls"):
                    tool_calls_buffer.extend(chunk["tool_calls"])
                    logger.debug(f"ğŸ”§ æ”¶åˆ°å·¥å…·è°ƒç”¨: {len(chunk['tool_calls'])} ä¸ª")
                
                # æ£€æŸ¥æ˜¯å¦ç»“æŸ
                if chunk.get("done"):
                    if tool_calls_buffer:
                        logger.info(f"ğŸ”§ æµå¼ç»“æŸï¼Œå¤„ç† {len(tool_calls_buffer)} ä¸ªå·¥å…·è°ƒç”¨")
                        from app.mcp import mcp_client
                        actual_user_id = user_id or ""
                        tool_results = await mcp_client.batch_call_tools(
                            user_id=actual_user_id,
                            tool_calls=tool_calls_buffer
                        )
                        # å°†å·¥å…·ç»“æœæ³¨å…¥åˆ°ä¸Šä¸‹æ–‡ä¸­
                        tool_context = mcp_client.build_tool_context(tool_results, format="markdown")
                        
                        # æ„å»ºæœ€ç»ˆæç¤ºè¯ï¼Œè¦æ±‚AIåŸºäºå·¥å…·ç»“æœå›ç­”
                        final_prompt = f"{prompt}\n\n{tool_context}\n\nè¯·åŸºäºä»¥ä¸Šå·¥å…·æŸ¥è¯¢ç»“æœï¼Œç»™å‡ºå®Œæ•´è¯¦ç»†çš„å›ç­”ã€‚"
                        final_messages = messages.copy()
                        final_messages.append({"role": "user", "content": final_prompt})
                        
                        # é€’å½’è°ƒç”¨ç”Ÿæˆæœ€ç»ˆç»“æœ
                        async for final_chunk in self._generate_with_tools(
                            final_messages, model, temperature, max_tokens, tools, user_id
                        ):
                            yield final_chunk
                    break
                
                # è¾“å‡ºæ–‡æœ¬å†…å®¹
                if chunk.get("content"):
                    yield chunk["content"]
            return
        
        # æ— å·¥å…·æ—¶æ™®é€šæµå¼ç”Ÿæˆ
        async for chunk in self.client.chat_completion_stream(
            messages=messages,
            model=model,
            temperature=temperature,
            max_tokens=max_tokens,
        ):
            # ç¡®ä¿åª yield å­—ç¬¦ä¸²å†…å®¹ï¼Œé¿å… yield å­—å…¸å¯¼è‡´ç±»å‹é”™è¯¯
            if isinstance(chunk, dict):
                if chunk.get("content"):
                    yield chunk["content"]
            else:
                yield chunk

    async def _generate_with_tools(
        self,
        messages: list,
        model: str,
        temperature: float,
        max_tokens: int,
        tools: list,
        user_id: Optional[str] = None,
    ) -> AsyncGenerator[str, None]:
        """è¾…åŠ©æ–¹æ³•ï¼šå¸¦å·¥å…·çš„æµå¼ç”Ÿæˆï¼ˆæ— tool_choiceï¼ŒAIè‡ªç”±å†³å®šï¼‰"""
        async for chunk in self.client.chat_completion_stream(
            messages=messages,
            model=model,
            temperature=temperature,
            max_tokens=max_tokens,
            tools=tools,
            tool_choice="auto",
        ):
            if chunk.get("tool_calls"):
                from app.mcp import mcp_client
                actual_user_id = user_id or ""
                tool_results = await mcp_client.batch_call_tools(
                    user_id=actual_user_id,
                    tool_calls=chunk["tool_calls"]
                )
                tool_context = mcp_client.build_tool_context(tool_results, format="markdown")
                
                # å†æ¬¡è°ƒç”¨è·å–æœ€ç»ˆå›ç­”
                messages.append({"role": "user", "content": f"{tool_context}\n\nè¯·åŸºäºä»¥ä¸Šå·¥å…·æŸ¥è¯¢ç»“æœï¼Œç»™å‡ºå®Œæ•´è¯¦ç»†çš„å›ç­”ã€‚"})
                
                async for final_chunk in self._generate_with_tools(
                    messages, model, temperature, max_tokens, tools, user_id
                ):
                    yield final_chunk
                break
            
            if chunk.get("done"):
                break
            
            if chunk.get("content"):
                yield chunk["content"]