"""AIæœåŠ¡å°è£… - ç»Ÿä¸€çš„AIæ¥å£

é‡æ„åæ”¯æŒè‡ªåŠ¨MCPå·¥å…·åŠ è½½ï¼š
- æ‰€æœ‰AIæ–¹æ³•åœ¨è¯·æ±‚å‰è‡ªåŠ¨æ£€æŸ¥ç”¨æˆ·MCPé…ç½®
- å¦‚æœæœ‰å¯ç”¨çš„MCPæ’ä»¶ä¸”æœ‰å¯ç”¨å·¥å…·ï¼Œè‡ªåŠ¨å‘é€tools
- é€šè¿‡ auto_mcp å‚æ•°æ§åˆ¶æ˜¯å¦å¯ç”¨è‡ªåŠ¨å·¥å…·åŠ è½½
"""
from typing import Optional, AsyncGenerator, List, Dict, Any, Union

from app.config import settings as app_settings
from app.logger import get_logger
from app.services.ai_config import AIClientConfig, default_config
from app.services.ai_clients.openai_client import OpenAIClient
from app.services.ai_clients.anthropic_client import AnthropicClient
from app.services.ai_clients.gemini_client import GeminiClient
from app.services.ai_clients.base_client import cleanup_all_clients
from app.services.ai_providers.openai_provider import OpenAIProvider
from app.services.ai_providers.anthropic_provider import AnthropicProvider
from app.services.ai_providers.gemini_provider import GeminiProvider
from app.services.ai_providers.base_provider import BaseAIProvider
from app.services.json_helper import clean_json_response, parse_json

# å¯¼å‡ºæ¸…ç†å‡½æ•°
cleanup_http_clients = cleanup_all_clients

logger = get_logger(__name__)


class AIService:
    """
    AIæœåŠ¡ç»Ÿä¸€æ¥å£
    
    MCPå·¥å…·æ”¯æŒï¼š
    - åœ¨åˆ›å»ºæœåŠ¡æ—¶ä¼ å…¥ user_id å’Œ db_session
    - æ ¹æ®ç”¨æˆ·MCPæ’ä»¶çš„enabledçŠ¶æ€è‡ªåŠ¨å†³å®šæ˜¯å¦å¯ç”¨MCP
    - å¦‚æœæœ‰ä»»æ„ä¸€ä¸ªMCPæ’ä»¶å¯ç”¨ï¼Œåˆ™åŠ è½½å¹¶ä½¿ç”¨å·¥å…·
    - å¦‚æœæ‰€æœ‰æ’ä»¶éƒ½å…³é—­ï¼Œåˆ™ä¸ä½¿ç”¨ä»»ä½•MCPå·¥å…·
    - é€šè¿‡ auto_mcp=False å¯ä¸´æ—¶ç¦ç”¨è‡ªåŠ¨å·¥å…·åŠ è½½
    - é€šè¿‡ mcp_max_rounds æ§åˆ¶å·¥å…·è°ƒç”¨è½®æ•°
    - é€šè¿‡ clear_mcp_cache() å¯æ¸…ç†MCPå·¥å…·ç¼“å­˜
    
    MCPå¯ç”¨é€»è¾‘ï¼ˆbackend/app/api/settings.py ä¸­çš„ get_user_ai_serviceï¼‰ï¼š
    - æŸ¥è¯¢ç”¨æˆ·çš„æ‰€æœ‰MCPæ’ä»¶
    - å¦‚æœæœ‰å¯ç”¨çš„æ’ä»¶ (enabled=True)ï¼Œåˆ™ enable_mcp=True
    - å¦‚æœæ‰€æœ‰æ’ä»¶éƒ½å…³é—­æˆ–æ²¡æœ‰æ’ä»¶ï¼Œåˆ™ enable_mcp=False
    
    ä½¿ç”¨ç¤ºä¾‹ï¼š
        # åˆ›å»ºæ”¯æŒMCPçš„AIæœåŠ¡ï¼ˆæ ¹æ®æ’ä»¶çŠ¶æ€è‡ªåŠ¨å†³å®šæ˜¯å¦å¯ç”¨ï¼‰
        ai_service = create_user_ai_service_with_mcp(
            api_provider="openai",
            api_key="...",
            user_id="user123",
            db_session=db
        )
        
        # è‡ªåŠ¨åŠ è½½MCPå·¥å…·ï¼ˆå¦‚æœæœ‰å¯ç”¨çš„æ’ä»¶ï¼‰
        result = await ai_service.generate_text(prompt="...")
        
        # ä¸´æ—¶ç¦ç”¨MCPå·¥å…·
        result = await ai_service.generate_text(prompt="...", auto_mcp=False)
        
        # è‡ªå®šä¹‰è½®æ•°
        result = await ai_service.generate_text(prompt="...", mcp_max_rounds=3)
    """

    def __init__(
        self,
        api_provider: Optional[str] = None,
        api_key: Optional[str] = None,
        api_base_url: Optional[str] = None,
        default_model: Optional[str] = None,
        default_temperature: Optional[float] = None,
        default_max_tokens: Optional[int] = None,
        default_system_prompt: Optional[str] = None,
        config: Optional[AIClientConfig] = None,
        # MCPæ”¯æŒå‚æ•°
        user_id: Optional[str] = None,
        db_session: Optional[Any] = None,
        enable_mcp: bool = True,
    ):
        self.api_provider = api_provider or app_settings.default_ai_provider
        self.default_model = default_model or app_settings.default_model
        self.default_temperature = default_temperature or app_settings.default_temperature
        self.default_max_tokens = default_max_tokens or app_settings.default_max_tokens
        self.default_system_prompt = default_system_prompt
        self.config = config or default_config
        
        # MCPé…ç½®
        self.user_id = user_id
        self.db_session = db_session
        self._enable_mcp = enable_mcp
        self._cached_tools: Optional[List[Dict]] = None
        self._tools_loaded = False
        
        self._openai_provider: Optional[OpenAIProvider] = None
        self._anthropic_provider: Optional[AnthropicProvider] = None
        self._gemini_provider: Optional[GeminiProvider] = None
        
        # åˆå§‹åŒ– OpenAI
        openai_key = api_key if api_provider == "openai" else app_settings.openai_api_key
        if openai_key:
            base_url = api_base_url if api_provider == "openai" else app_settings.openai_base_url
            client = OpenAIClient(openai_key, base_url or "https://api.openai.com/v1", self.config)
            self._openai_provider = OpenAIProvider(client)
        
        # åˆå§‹åŒ– Anthropic
        anthropic_key = api_key if api_provider == "anthropic" else app_settings.anthropic_api_key
        if anthropic_key:
            base_url = api_base_url if api_provider == "anthropic" else app_settings.anthropic_base_url
            client = AnthropicClient(anthropic_key, base_url, self.config)
            self._anthropic_provider = AnthropicProvider(client)
        
        # åˆå§‹åŒ– Gemini
        if api_provider == "gemini" and api_key:
            client = GeminiClient(api_key, api_base_url, self.config)
            self._gemini_provider = GeminiProvider(client)

    @property
    def enable_mcp(self) -> bool:
        """æ˜¯å¦å¯ç”¨MCPå·¥å…·"""
        return self._enable_mcp
    
    @enable_mcp.setter
    def enable_mcp(self, value: bool):
        """è®¾ç½®MCPå¯ç”¨çŠ¶æ€ï¼Œå¦‚æœç¦ç”¨åˆ™æ¸…ç†ç¼“å­˜"""
        if value is False and self._enable_mcp is True:
            # ä»å¯ç”¨å˜ä¸ºç¦ç”¨ï¼Œæ¸…ç†ç¼“å­˜
            self.clear_mcp_cache()
        self._enable_mcp = value
    
    def clear_mcp_cache(self):
        """
        æ¸…ç†MCPå·¥å…·ç¼“å­˜
        
        å½“ç¦ç”¨MCPæ—¶è°ƒç”¨æ­¤æ–¹æ³•ï¼Œç¡®ä¿åç»­AIè°ƒç”¨ä¸ä¼šä½¿ç”¨ç¼“å­˜çš„å·¥å…·ã€‚
        åŒæ—¶æ›´æ–° _tools_loaded çŠ¶æ€ï¼Œä½¿ä¸‹æ¬¡è°ƒç”¨æ—¶é‡æ–°æ£€æŸ¥ã€‚
        """
        if self._cached_tools is not None:
            logger.info(f"ğŸ”§ æ¸…ç†MCPå·¥å…·ç¼“å­˜ï¼Œç§»é™¤ {len(self._cached_tools)} ä¸ªå·¥å…·")
            self._cached_tools = None
        else:
            logger.debug(f"ğŸ”§ MCPå·¥å…·ç¼“å­˜å·²ç»æ˜¯ç©ºï¼Œæ— éœ€æ¸…ç†")
        
        # æ›´æ–°åŠ è½½çŠ¶æ€ï¼Œç¡®ä¿ä¸‹æ¬¡è°ƒç”¨ä¼šé‡æ–°æ£€æŸ¥
        self._tools_loaded = False
        logger.debug(f"ğŸ”§ MCPå·¥å…·çŠ¶æ€å·²é‡ç½®: enable_mcp={self._enable_mcp}, _tools_loaded=False")
    
    def _get_provider(self, provider: Optional[str] = None) -> BaseAIProvider:
        """è·å–å¯¹åº”çš„ Provider"""
        p = provider or self.api_provider
        if p == "openai" and self._openai_provider:
            return self._openai_provider
        if p == "anthropic" and self._anthropic_provider:
            return self._anthropic_provider
        if p == "gemini" and self._gemini_provider:
            return self._gemini_provider
        raise ValueError(f"Provider {p} æœªåˆå§‹åŒ–")

    async def _prepare_mcp_tools(self, auto_mcp: bool = True, force_refresh: bool = False) -> Optional[List[Dict]]:
        """
        é¢„å¤„ç†MCPå·¥å…·
        
        æ£€æŸ¥ç”¨æˆ·MCPé…ç½®å¹¶åŠ è½½å¯ç”¨å·¥å…·ã€‚
        ç»“æœä¼šè¢«ç¼“å­˜ï¼Œé¿å…é‡å¤åŠ è½½ã€‚
        
        Args:
            auto_mcp: æ˜¯å¦è‡ªåŠ¨åŠ è½½MCPå·¥å…·ï¼ˆæ¥è‡ªè°ƒç”¨æ–¹å‚æ•°ï¼‰
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ç¼“å­˜
            
        Returns:
            - None: æ— å¯ç”¨å·¥å…·ï¼ˆæœªé…ç½®/æœªå¯ç”¨/åŠ è½½å¤±è´¥ï¼‰
            - List[Dict]: OpenAIæ ¼å¼çš„å·¥å…·åˆ—è¡¨
        """
        # å‰ç½®æ¡ä»¶æ£€æŸ¥
        if not self._enable_mcp:
            logger.debug(f"ğŸ”§ MCPå·¥å…·æœªå¯ç”¨ (_enable_mcp=False)")
            # å³ä½¿æœ‰ç¼“å­˜ä¹Ÿæ¸…ç†æ‰ï¼Œç¡®ä¿ä¸ä½¿ç”¨
            self._cached_tools = None
            self._tools_loaded = False
            return None
        
        if not auto_mcp:
            logger.debug(f"ğŸ”§ auto_mcp=Falseï¼Œè·³è¿‡MCPå·¥å…·åŠ è½½")
            # å³ä½¿æœ‰ç¼“å­˜ä¹Ÿæ¸…ç†æ‰ï¼Œç¡®ä¿ä¸ä½¿ç”¨
            self._cached_tools = None
            self._tools_loaded = False
            return None
        
        if not self.user_id:
            logger.debug(f"ğŸ”§ MCPå·¥å…·åŠ è½½è·³è¿‡: user_idæœªè®¾ç½®")
            return None
        
        if not self.db_session:
            logger.debug(f"ğŸ”§ MCPå·¥å…·åŠ è½½è·³è¿‡: db_sessionæœªè®¾ç½®")
            return None
        
        # ä½¿ç”¨ç¼“å­˜ï¼ˆåªæœ‰ enable_mcp=True æ—¶æ‰ä½¿ç”¨ç¼“å­˜ï¼‰
        if self._tools_loaded and not force_refresh:
            if self._cached_tools:
                logger.debug(f"ğŸ”§ ä½¿ç”¨ç¼“å­˜çš„MCPå·¥å…· ({len(self._cached_tools)}ä¸ª)")
            return self._cached_tools
        
        try:
            from app.services.mcp_tools_loader import mcp_tools_loader
            
            self._cached_tools = await mcp_tools_loader.get_user_tools(
                user_id=self.user_id,
                db_session=self.db_session,
                use_cache=True,
                force_refresh=force_refresh
            )
            self._tools_loaded = True
            
            if self._cached_tools:
                logger.info(f"ğŸ”§ å·²åŠ è½½ {len(self._cached_tools)} ä¸ªMCPå·¥å…·")
            else:
                logger.debug(f"ğŸ“­ ç”¨æˆ· {self.user_id} æ²¡æœ‰å¯ç”¨çš„MCPå·¥å…·")
            
            return self._cached_tools
            
        except Exception as e:
            logger.warning(f"âš ï¸ åŠ è½½MCPå·¥å…·å¤±è´¥: {e}")
            self._tools_loaded = True
            self._cached_tools = None
            return None

    async def _handle_tool_calls(
        self,
        original_prompt: str,
        response: Dict[str, Any],
        max_rounds: int = 2,
        **kwargs
    ) -> Dict[str, Any]:
        """
        å¤„ç†AIè¿”å›çš„å·¥å…·è°ƒç”¨
        
        Args:
            original_prompt: åŸå§‹æç¤ºè¯
            response: AIå“åº”ï¼ˆåŒ…å«tool_callsï¼‰
            max_rounds: æœ€å¤§å·¥å…·è°ƒç”¨è½®æ•°
            **kwargs: ä¼ é€’ç»™generate_textçš„å…¶ä»–å‚æ•°
            
        Returns:
            æœ€ç»ˆçš„AIå“åº”
        """
        from app.mcp import mcp_client
        
        tool_calls = response.get("tool_calls", [])
        if not tool_calls or not self.user_id:
            return response
        
        result = {
            "content": response.get("content", ""),
            "tool_calls_made": 0,
            "tools_used": [],
            "finish_reason": response.get("finish_reason", ""),
            "mcp_enhanced": True
        }
        
        prompt = original_prompt
        
        for round_num in range(max_rounds):
            logger.info(f"ğŸ”§ å·¥å…·è°ƒç”¨ - ç¬¬{round_num+1}/{max_rounds}è½®ï¼Œ{len(tool_calls)}ä¸ªå·¥å…·")
            
            try:
                # æ‰¹é‡æ‰§è¡Œå·¥å…·è°ƒç”¨
                tool_results = await mcp_client.batch_call_tools(
                    user_id=self.user_id,
                    tool_calls=tool_calls
                )
                
                # è®°å½•ä½¿ç”¨çš„å·¥å…·
                for tc in tool_calls:
                    name = tc["function"]["name"]
                    if name not in result["tools_used"]:
                        result["tools_used"].append(name)
                result["tool_calls_made"] += len(tool_calls)
                
                # æ„å»ºå·¥å…·ä¸Šä¸‹æ–‡
                tool_context = mcp_client.build_tool_context(tool_results, format="markdown")
                
                # æ›´æ–°æç¤ºè¯
                if round_num == max_rounds - 1:
                    # æœ€åä¸€è½®ï¼Œå¼ºåˆ¶è¦æ±‚å›ç­”
                    prompt = f"{original_prompt}\n\n{tool_context}\n\nâš ï¸ é‡è¦ï¼šè¯·åŸºäºä»¥ä¸Šå·¥å…·æŸ¥è¯¢ç»“æœï¼Œç»™å‡ºå®Œæ•´è¯¦ç»†çš„æœ€ç»ˆç­”æ¡ˆã€‚ä¸è¦å†è°ƒç”¨å·¥å…·ã€‚"
                    tool_choice = "none"
                else:
                    prompt = f"{original_prompt}\n\n{tool_context}\n\nè¯·åŸºäºä»¥ä¸Šå·¥å…·æŸ¥è¯¢ç»“æœï¼Œç»§ç»­å®Œæˆä»»åŠ¡ã€‚"
                    tool_choice = kwargs.get("tool_choice", "auto")
                
                # ç»§ç»­è°ƒç”¨AI
                prov = self._get_provider(kwargs.get("provider"))
                next_response = await prov.generate(
                    prompt=prompt,
                    model=kwargs.get("model") or self.default_model,
                    temperature=kwargs.get("temperature") or self.default_temperature,
                    max_tokens=kwargs.get("max_tokens") or self.default_max_tokens,
                    system_prompt=kwargs.get("system_prompt") or self.default_system_prompt,
                    tools=None if tool_choice == "none" else self._cached_tools,
                    tool_choice=tool_choice,
                )
                
                tool_calls = next_response.get("tool_calls", [])
                
                if not tool_calls:
                    # æ²¡æœ‰æ›´å¤šå·¥å…·è°ƒç”¨ï¼Œè¿”å›ç»“æœ
                    result["content"] = next_response.get("content", "")
                    result["finish_reason"] = next_response.get("finish_reason", "stop")
                    break
                    
            except Exception as e:
                logger.error(f"âŒ å·¥å…·è°ƒç”¨å¤±è´¥: {e}")
                result["content"] = response.get("content", "")
                result["finish_reason"] = "tool_error"
                break
        
        return result

    async def generate_text(
        self,
        prompt: str,
        provider: Optional[str] = None,
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        system_prompt: Optional[str] = None,
        tools: Optional[List[Dict]] = None,
        tool_choice: Optional[str] = None,
        auto_mcp: bool = True,
        handle_tool_calls: bool = True,
        mcp_max_rounds: Optional[int] = None,
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ–‡æœ¬ï¼ˆè‡ªåŠ¨æ”¯æŒMCPå·¥å…·ï¼‰
        
        Args:
            prompt: ç”¨æˆ·æç¤ºè¯
            provider: AIæä¾›å•†
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦
            max_tokens: æœ€å¤§ä»¤ç‰Œæ•°
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            tools: æ‰‹åŠ¨æŒ‡å®šçš„å·¥å…·åˆ—è¡¨ï¼ˆä¼˜å…ˆçº§é«˜äºè‡ªåŠ¨åŠ è½½ï¼‰
            tool_choice: å·¥å…·é€‰æ‹©ç­–ç•¥
            auto_mcp: æ˜¯å¦è‡ªåŠ¨åŠ è½½MCPå·¥å…·ï¼ˆé»˜è®¤Trueï¼‰
            handle_tool_calls: æ˜¯å¦è‡ªåŠ¨å¤„ç†å·¥å…·è°ƒç”¨ï¼ˆé»˜è®¤Trueï¼‰
            mcp_max_rounds: æœ€å¤§å·¥å…·è°ƒç”¨è½®æ•°ï¼ˆNoneä½¿ç”¨é»˜è®¤å€¼3ï¼‰
            
        Returns:
            åŒ…å«ç”Ÿæˆå†…å®¹çš„å­—å…¸
        """
        # ä½¿ç”¨å…¨å±€é…ç½®çš„MCPè½®æ•°ï¼ˆå¦‚æœæœªæŒ‡å®šï¼‰
        if mcp_max_rounds is None:
            mcp_max_rounds = app_settings.mcp_max_rounds
        
        # è‡ªåŠ¨åŠ è½½MCPå·¥å…·
        if auto_mcp and tools is None:
            tools = await self._prepare_mcp_tools(auto_mcp=auto_mcp)
        
        prov = self._get_provider(provider)
        response = await prov.generate(
            prompt=prompt,
            model=model or self.default_model,
            temperature=temperature or self.default_temperature,
            max_tokens=max_tokens or self.default_max_tokens,
            system_prompt=system_prompt or self.default_system_prompt,
            tools=tools,
            tool_choice=tool_choice,
        )
        
        # å¤„ç†å·¥å…·è°ƒç”¨
        if handle_tool_calls and response.get("tool_calls"):
            return await self._handle_tool_calls(
                original_prompt=prompt,
                response=response,
                provider=provider,
                model=model,
                temperature=temperature,
                max_tokens=max_tokens,
                system_prompt=system_prompt,
                tool_choice=tool_choice,
                max_rounds=mcp_max_rounds,
            )
        
        return response

    async def generate_text_stream(
        self,
        prompt: str,
        provider: Optional[str] = None,
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        system_prompt: Optional[str] = None,
        tool_choice: Optional[str] = None,
        auto_mcp: bool = True,
        mcp_max_rounds: Optional[int] = None,
    ) -> AsyncGenerator[str, None]:
        """
        æµå¼ç”Ÿæˆæ–‡æœ¬ï¼ˆè‡ªåŠ¨æ”¯æŒMCPå·¥å…·ï¼‰
        
        å·¥å…·è°ƒç”¨åœ¨ Provider å±‚é€šè¿‡æµå¼æ–¹å¼å¤„ç†ï¼Œæ”¯æŒçœŸæ­£çš„æµå¼å·¥å…·è°ƒç”¨ã€‚
        
        Args:
            prompt: ç”¨æˆ·æç¤ºè¯
            provider: AIæä¾›å•†
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦
            max_tokens: æœ€å¤§ä»¤ç‰Œæ•°
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            tool_choice: å·¥å…·é€‰æ‹©ç­–ç•¥ï¼ˆ"auto"/"none"/"required"ï¼‰
            auto_mcp: æ˜¯å¦è‡ªåŠ¨åŠ è½½MCPå·¥å…·
            mcp_max_rounds: æœ€å¤§å·¥å…·è°ƒç”¨è½®æ•°ï¼ˆNoneä½¿ç”¨é»˜è®¤å€¼3ï¼‰
            
        Yields:
            ç”Ÿæˆçš„æ–‡æœ¬å—
        """
        logger.debug(f"ğŸ”§ generate_text_stream: auto_mcp={auto_mcp}, tool_choice={tool_choice}")
        
        tools_to_use = None
        
        # åŠ è½½MCPå·¥å…·
        if auto_mcp:
            tools_to_use = await self._prepare_mcp_tools(auto_mcp=auto_mcp)
            if tools_to_use:
                logger.info(f"ğŸ”§ å·²è·å– {len(tools_to_use)} ä¸ªMCPå·¥å…·")
        
        # æµå¼ç”Ÿæˆï¼ˆProvider å±‚å¤„ç†å·¥å…·è°ƒç”¨ï¼‰
        prov = self._get_provider(provider)
        logger.debug(f"ğŸ”§ å¼€å§‹æµå¼ç”Ÿæˆï¼Œprovider={provider or self.api_provider}, tools_count={len(tools_to_use) if tools_to_use else 0}")
        async for chunk in prov.generate_stream(
            prompt=prompt,
            model=model or self.default_model,
            temperature=temperature or self.default_temperature,
            max_tokens=max_tokens or self.default_max_tokens,
            system_prompt=system_prompt or self.default_system_prompt,
            tools=tools_to_use,
            tool_choice=tool_choice,
            user_id=self.user_id,
        ):
            yield chunk

    async def call_with_json_retry(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        max_retries: int = 3,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        provider: Optional[str] = None,
        model: Optional[str] = None,
        expected_type: Optional[str] = None,
        auto_mcp: bool = True,
    ) -> Union[Dict, List]:
        """
        å¸¦é‡è¯•çš„ JSON è°ƒç”¨ï¼ˆè‡ªåŠ¨æ”¯æŒMCPå·¥å…·ï¼‰
        
        Args:
            prompt: ç”¨æˆ·æç¤ºè¯
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            temperature: æ¸©åº¦
            max_tokens: æœ€å¤§ä»¤ç‰Œæ•°
            provider: AIæä¾›å•†
            model: æ¨¡å‹åç§°
            expected_type: æœŸæœ›çš„è¿”å›ç±»å‹ï¼ˆ"object"æˆ–"array"ï¼‰
            auto_mcp: æ˜¯å¦è‡ªåŠ¨åŠ è½½MCPå·¥å…·
            
        Returns:
            è§£æåçš„JSONæ•°æ®
        """
        last_response = ""
        
        for attempt in range(1, max_retries + 1):
            current_prompt = prompt if attempt == 1 else self._add_json_hint(prompt, last_response, attempt)
            
            result = await self.generate_text(
                prompt=current_prompt,
                provider=provider,
                model=model,
                temperature=temperature,
                max_tokens=max_tokens,
                system_prompt=system_prompt,
                auto_mcp=auto_mcp,
                handle_tool_calls=True,
            )
            
            last_response = result.get("content", "")
            
            try:
                data = parse_json(last_response)
                if expected_type == "object" and not isinstance(data, dict):
                    raise ValueError("æœŸæœ›å¯¹è±¡")
                if expected_type == "array" and not isinstance(data, list):
                    raise ValueError("æœŸæœ›æ•°ç»„")
                return data
            except Exception as e:
                if attempt == max_retries:
                    raise ValueError(f"JSON è§£æå¤±è´¥: {e}")
        
        raise ValueError("JSON è°ƒç”¨å¤±è´¥")

    @staticmethod
    def _add_json_hint(prompt: str, failed: str, attempt: int) -> str:
        return f"{prompt}\n\nâš ï¸ ç¬¬{attempt}æ¬¡é‡è¯•ï¼Œè¯·è¿”å›çº¯JSONï¼Œä¸è¦markdownåŒ…è£¹ã€‚ä¸Šæ¬¡é”™è¯¯: {failed[:200]}..."

    @staticmethod
    def _clean_json_response(text: str) -> str:
        """æ¸…æ´— JSON å“åº”"""
        return clean_json_response(text)


def create_user_ai_service(
    api_provider: str,
    api_key: str,
    api_base_url: str,
    model_name: str,
    temperature: float,
    max_tokens: int,
    system_prompt: Optional[str] = None,
) -> AIService:
    """åˆ›å»ºç”¨æˆ· AI æœåŠ¡ï¼ˆä¸å¸¦MCPæ”¯æŒï¼‰"""
    return AIService(
        api_provider=api_provider,
        api_key=api_key,
        api_base_url=api_base_url,
        default_model=model_name,
        default_temperature=temperature,
        default_max_tokens=max_tokens,
        default_system_prompt=system_prompt,
    )


def create_user_ai_service_with_mcp(
    api_provider: str,
    api_key: str,
    api_base_url: str,
    model_name: str,
    temperature: float,
    max_tokens: int,
    user_id: str,
    db_session,
    system_prompt: Optional[str] = None,
    enable_mcp: bool = True,
) -> AIService:
    """
    åˆ›å»ºæ”¯æŒMCPçš„ç”¨æˆ·AIæœåŠ¡
    
    Args:
        api_provider: AIæä¾›å•†
        api_key: APIå¯†é’¥
        api_base_url: APIåŸºç¡€URL
        model_name: æ¨¡å‹åç§°
        temperature: æ¸©åº¦
        max_tokens: æœ€å¤§ä»¤ç‰Œæ•°
        user_id: ç”¨æˆ·IDï¼ˆç”¨äºåŠ è½½MCPå·¥å…·ï¼‰
        db_session: æ•°æ®åº“ä¼šè¯
        system_prompt: ç³»ç»Ÿæç¤ºè¯
        enable_mcp: æ˜¯å¦å¯ç”¨MCPå·¥å…·
        
    Returns:
        é…ç½®å¥½çš„AIServiceå®ä¾‹
    """
    return AIService(
        api_provider=api_provider,
        api_key=api_key,
        api_base_url=api_base_url,
        default_model=model_name,
        default_temperature=temperature,
        default_max_tokens=max_tokens,
        default_system_prompt=system_prompt,
        user_id=user_id,
        db_session=db_session,
        enable_mcp=enable_mcp,
    )