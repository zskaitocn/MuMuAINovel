"""å‰§æƒ…åˆ†ææœåŠ¡ - è‡ªåŠ¨åˆ†æç« èŠ‚çš„é’©å­ã€ä¼ç¬”ã€å†²çªç­‰å…ƒç´ """
from typing import Dict, Any, List, Optional
from sqlalchemy.ext.asyncio import AsyncSession
from app.services.ai_service import AIService
from app.services.prompt_service import prompt_service, PromptService
from app.logger import get_logger
import json
import re
import asyncio

logger = get_logger(__name__)


class PlotAnalyzer:
    """å‰§æƒ…åˆ†æå™¨ - ä½¿ç”¨AIåˆ†æç« èŠ‚å†…å®¹"""
    
    def __init__(self, ai_service: AIService):
        """
        åˆå§‹åŒ–å‰§æƒ…åˆ†æå™¨
        
        Args:
            ai_service: AIæœåŠ¡å®ä¾‹
        """
        self.ai_service = ai_service
        logger.info("âœ… PlotAnalyzeråˆå§‹åŒ–æˆåŠŸ")
    
    async def analyze_chapter(
        self,
        chapter_number: int,
        title: str,
        content: str,
        word_count: int,
        user_id: str = None,
        db: AsyncSession = None,
        max_retries: int = 3
    ) -> Optional[Dict[str, Any]]:
        """
        åˆ†æå•ç« å†…å®¹ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        
        Args:
            chapter_number: ç« èŠ‚å·
            title: ç« èŠ‚æ ‡é¢˜
            content: ç« èŠ‚å†…å®¹
            word_count: å­—æ•°
            user_id: ç”¨æˆ·IDï¼ˆç”¨äºè·å–è‡ªå®šä¹‰æç¤ºè¯ï¼‰
            db: æ•°æ®åº“ä¼šè¯ï¼ˆç”¨äºæŸ¥è¯¢è‡ªå®šä¹‰æç¤ºè¯ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤3æ¬¡
        
        Returns:
            åˆ†æç»“æœå­—å…¸,å¤±è´¥è¿”å›None
        """
        logger.info(f"ğŸ” å¼€å§‹åˆ†æç¬¬{chapter_number}ç« : {title}")
        
        # å¦‚æœå†…å®¹è¿‡é•¿,æˆªå–å‰8000å­—(é¿å…è¶…token)
        analysis_content = content[:8000] if len(content) > 8000 else content
        
        # è·å–è‡ªå®šä¹‰æç¤ºè¯æ¨¡æ¿
        try:
            if user_id and db:
                template = await PromptService.get_template("PLOT_ANALYSIS", user_id, db)
            else:
                # é™çº§åˆ°ç³»ç»Ÿé»˜è®¤æ¨¡æ¿
                template = PromptService.PLOT_ANALYSIS
        except Exception as e:
            logger.warning(f"âš ï¸ è·å–æç¤ºè¯æ¨¡æ¿å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿: {str(e)}")
            template = PromptService.PLOT_ANALYSIS
        
        # æ ¼å¼åŒ–æç¤ºè¯
        prompt = PromptService.format_prompt(
            template,
            chapter_number=chapter_number,
            title=title,
            word_count=word_count,
            content=analysis_content
        )
        
        last_error = None
        
        for attempt in range(1, max_retries + 1):
            try:
                # è°ƒç”¨AIè¿›è¡Œåˆ†æ
                logger.info(f"  ğŸ“¡ è°ƒç”¨AIåˆ†æ(å†…å®¹é•¿åº¦: {len(analysis_content)}å­—, å°è¯• {attempt}/{max_retries})...")
                accumulated_text = ""
                
                try:
                    async for chunk in self.ai_service.generate_text_stream(
                        prompt=prompt,
                        temperature=0.3  # é™ä½æ¸©åº¦ä»¥è·å¾—æ›´ç¨³å®šçš„JSONè¾“å‡º
                    ):
                        accumulated_text += chunk
                except GeneratorExit:
                    # æµå¼å“åº”è¢«ä¸­æ–­
                    logger.warning(f"âš ï¸ æµå¼å“åº”è¢«ä¸­æ–­(GeneratorExit)ï¼Œå·²ç´¯ç§¯ {len(accumulated_text)} å­—ç¬¦")
                    # å¦‚æœå·²ç»ç´¯ç§¯äº†è¶³å¤Ÿå†…å®¹ï¼Œç»§ç»­å°è¯•è§£æ
                    if len(accumulated_text) < 100:
                        raise Exception("æµå¼å“åº”ä¸­æ–­ï¼Œå†…å®¹ä¸è¶³")
                except Exception as stream_error:
                    logger.error(f"âŒ æµå¼ç”Ÿæˆå‡ºé”™: {str(stream_error)}")
                    raise
                
                # æ£€æŸ¥å“åº”æ˜¯å¦ä¸ºç©º
                if not accumulated_text or len(accumulated_text.strip()) < 10:
                    logger.warning(f"âš ï¸ AIå“åº”ä¸ºç©ºæˆ–è¿‡çŸ­(é•¿åº¦: {len(accumulated_text)}), å°è¯• {attempt}/{max_retries}")
                    last_error = "AIå“åº”ä¸ºç©ºæˆ–è¿‡çŸ­"
                    if attempt < max_retries:
                        wait_time = min(2 ** attempt, 10)
                        logger.info(f"  â³ ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                        await asyncio.sleep(wait_time)
                        continue
                    else:
                        logger.error(f"âŒ ç¬¬{chapter_number}ç« åˆ†æå¤±è´¥: AIå“åº”ä¸ºç©ºï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°")
                        return None
                
                # æå–å†…å®¹
                response_text = accumulated_text
                logger.debug(f"  æ”¶åˆ°AIå“åº”ï¼Œé•¿åº¦: {len(response_text)} å­—ç¬¦")
                
                # è§£æJSONç»“æœ
                analysis_result = self._parse_analysis_response(response_text)
                
                if analysis_result:
                    logger.info(f"âœ… ç¬¬{chapter_number}ç« åˆ†æå®Œæˆ (å°è¯• {attempt}/{max_retries})")
                    logger.info(f"  - é’©å­: {len(analysis_result.get('hooks', []))}ä¸ª")
                    logger.info(f"  - ä¼ç¬”: {len(analysis_result.get('foreshadows', []))}ä¸ª")
                    logger.info(f"  - æƒ…èŠ‚ç‚¹: {len(analysis_result.get('plot_points', []))}ä¸ª")
                    logger.info(f"  - æ•´ä½“è¯„åˆ†: {analysis_result.get('scores', {}).get('overall', 'N/A')}")
                    return analysis_result
                else:
                    # JSONè§£æå¤±è´¥ï¼Œé‡è¯•
                    logger.warning(f"âš ï¸ JSONè§£æå¤±è´¥, å°è¯• {attempt}/{max_retries}")
                    last_error = "JSONè§£æå¤±è´¥"
                    if attempt < max_retries:
                        wait_time = min(2 ** attempt, 10)
                        logger.info(f"  â³ ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                        await asyncio.sleep(wait_time)
                        continue
                    else:
                        logger.error(f"âŒ ç¬¬{chapter_number}ç« åˆ†æå¤±è´¥: JSONè§£æé”™è¯¯ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°")
                        return None
                    
            except Exception as e:
                last_error = str(e)
                logger.error(f"âŒ ç« èŠ‚åˆ†æå¼‚å¸¸(å°è¯• {attempt}/{max_retries}): {last_error}")
                
                if attempt < max_retries:
                    wait_time = min(2 ** attempt, 10)
                    logger.info(f"  â³ ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    await asyncio.sleep(wait_time)
                    continue
                else:
                    logger.error(f"âŒ ç¬¬{chapter_number}ç« åˆ†æå¤±è´¥: {last_error}ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°")
                    return None
        
        # ä¸åº”è¯¥åˆ°è¾¾è¿™é‡Œï¼Œä½†ä½œä¸ºå®‰å…¨æªæ–½
        logger.error(f"âŒ ç¬¬{chapter_number}ç« åˆ†æå¤±è´¥: {last_error}")
        return None
    
    def _parse_analysis_response(self, response: str) -> Optional[Dict[str, Any]]:
        """
        è§£æAIè¿”å›çš„åˆ†æç»“æœï¼ˆä½¿ç”¨ç»Ÿä¸€çš„JSONæ¸…æ´—æ–¹æ³•ï¼‰
        
        Args:
            response: AIè¿”å›çš„æ–‡æœ¬
        
        Returns:
            è§£æåçš„å­—å…¸,å¤±è´¥è¿”å›None
        """
        try:
            # ä½¿ç”¨ç»Ÿä¸€çš„JSONæ¸…æ´—æ–¹æ³•
            cleaned = self.ai_service._clean_json_response(response)
            
            # å°è¯•è§£æJSON
            result = json.loads(cleaned)
            
            # éªŒè¯å¿…è¦å­—æ®µ
            required_fields = ['hooks', 'plot_points', 'scores']
            for field in required_fields:
                if field not in result:
                    logger.warning(f"âš ï¸ åˆ†æç»“æœç¼ºå°‘å­—æ®µ: {field}")
                    result[field] = [] if field != 'scores' else {}
            
            logger.info("âœ… æˆåŠŸè§£æåˆ†æç»“æœ")
            return result
            
        except json.JSONDecodeError as e:
            logger.error(f"âŒ JSONè§£æå¤±è´¥: {str(e)}")
            logger.error(f"  åŸå§‹å“åº”(å‰500å­—): {response[:500]}")
            return None
        except Exception as e:
            logger.error(f"âŒ è§£æå¼‚å¸¸: {str(e)}")
            return None
    
    def extract_memories_from_analysis(
        self,
        analysis: Dict[str, Any],
        chapter_id: str,
        chapter_number: int,
        chapter_content: str = "",
        chapter_title: str = ""
    ) -> List[Dict[str, Any]]:
        """
        ä»åˆ†æç»“æœä¸­æå–è®°å¿†ç‰‡æ®µ
        
        Args:
            analysis: åˆ†æç»“æœ
            chapter_id: ç« èŠ‚ID
            chapter_number: ç« èŠ‚å·
            chapter_content: ç« èŠ‚å®Œæ•´å†…å®¹(ç”¨äºè®¡ç®—ä½ç½®)
            chapter_title: ç« èŠ‚æ ‡é¢˜
        
        Returns:
            è®°å¿†ç‰‡æ®µåˆ—è¡¨
        """
        memories = []
        
        try:
            # ã€æ–°å¢ã€‘0. æå–ç« èŠ‚æ‘˜è¦ä½œä¸ºè®°å¿†ï¼ˆç”¨äºè¯­ä¹‰æ£€ç´¢ç›¸å…³ç« èŠ‚ï¼‰
            chapter_summary = ""
            
            # å°è¯•ä»åˆ†æç»“æœè·å–æ‘˜è¦
            if analysis.get('summary'):
                chapter_summary = analysis.get('summary')
            # æˆ–è€…ä»æƒ…èŠ‚ç‚¹ç»„åˆç”Ÿæˆæ‘˜è¦
            elif analysis.get('plot_points'):
                plot_summaries = [p.get('content', '') for p in analysis.get('plot_points', [])[:3]]
                chapter_summary = "ï¼›".join(plot_summaries)
            # æˆ–è€…ä½¿ç”¨å†…å®¹å‰300å­—
            elif chapter_content:
                chapter_summary = chapter_content[:300] + ("..." if len(chapter_content) > 300 else "")
            
            # å¦‚æœæœ‰æ‘˜è¦ï¼Œæ·»åŠ åˆ°è®°å¿†ä¸­
            if chapter_summary:
                memories.append({
                    'type': 'chapter_summary',
                    'content': chapter_summary,
                    'title': f"ç¬¬{chapter_number}ç« ã€Š{chapter_title}ã€‹æ‘˜è¦",
                    'metadata': {
                        'chapter_id': chapter_id,
                        'chapter_number': chapter_number,
                        'importance_score': 0.6,  # ä¸­ç­‰é‡è¦æ€§
                        'tags': ['æ‘˜è¦', 'ç« èŠ‚æ¦‚è§ˆ', chapter_title],
                        'is_foreshadow': 0,
                        'text_position': 0,
                        'text_length': len(chapter_summary)
                    }
                })
                logger.info(f"  âœ… æ·»åŠ ç« èŠ‚æ‘˜è¦è®°å¿†: {len(chapter_summary)}å­—")
            
            # 1. æå–é’©å­ä½œä¸ºè®°å¿†
            for i, hook in enumerate(analysis.get('hooks', [])):
                if hook.get('strength', 0) >= 6:  # åªä¿å­˜å¼ºåº¦>=6çš„é’©å­
                    keyword = hook.get('keyword', '')
                    position, length = self._find_text_position(chapter_content, keyword)
                    
                    logger.info(f"  é’©å­ä½ç½®: keyword='{keyword[:30]}...', pos={position}, len={length}")
                    
                    memories.append({
                        'type': 'hook',
                        'content': f"[{hook.get('type', 'æœªçŸ¥')}é’©å­] {hook.get('content', '')}",
                        'title': f"{hook.get('type', 'é’©å­')} - {hook.get('position', '')}",
                        'metadata': {
                            'chapter_id': chapter_id,
                            'chapter_number': chapter_number,
                            'importance_score': min(hook.get('strength', 5) / 10, 1.0),
                            'tags': [hook.get('type', 'é’©å­'), hook.get('position', '')],
                            'is_foreshadow': 0,
                            'keyword': keyword,
                            'text_position': position,
                            'text_length': length,
                            'strength': hook.get('strength', 5),
                            'position_desc': hook.get('position', '')
                        }
                    })
            
            # 2. æå–ä¼ç¬”ä½œä¸ºè®°å¿†
            for i, foreshadow in enumerate(analysis.get('foreshadows', [])):
                is_planted = foreshadow.get('type') == 'planted'
                keyword = foreshadow.get('keyword', '')
                position, length = self._find_text_position(chapter_content, keyword)
                
                logger.info(f"  ä¼ç¬”ä½ç½®: keyword='{keyword[:30]}...', pos={position}, len={length}")
                
                memories.append({
                    'type': 'foreshadow',
                    'content': foreshadow.get('content', ''),
                    'title': f"{'åŸ‹ä¸‹ä¼ç¬”' if is_planted else 'å›æ”¶ä¼ç¬”'}",
                    'metadata': {
                        'chapter_id': chapter_id,
                        'chapter_number': chapter_number,
                        'importance_score': min(foreshadow.get('strength', 5) / 10, 1.0),
                        'tags': ['ä¼ç¬”', foreshadow.get('type', 'planted')],
                        'is_foreshadow': 1 if is_planted else 2,
                        'reference_chapter': foreshadow.get('reference_chapter'),
                        'keyword': keyword,
                        'text_position': position,
                        'text_length': length,
                        'foreshadow_type': foreshadow.get('type', 'planted'),
                        'strength': foreshadow.get('strength', 5)
                    }
                })
            
            # 3. æå–å…³é”®æƒ…èŠ‚ç‚¹
            for i, plot_point in enumerate(analysis.get('plot_points', [])):
                if plot_point.get('importance', 0) >= 0.6:  # åªä¿å­˜é‡è¦æ€§>=0.6çš„æƒ…èŠ‚ç‚¹
                    keyword = plot_point.get('keyword', '')
                    position, length = self._find_text_position(chapter_content, keyword)
                    
                    logger.info(f"  æƒ…èŠ‚ç‚¹ä½ç½®: keyword='{keyword[:30]}...', pos={position}, len={length}")
                    
                    memories.append({
                        'type': 'plot_point',
                        'content': f"{plot_point.get('content', '')}ã€‚å½±å“: {plot_point.get('impact', '')}",
                        'title': f"æƒ…èŠ‚ç‚¹ - {plot_point.get('type', 'æœªçŸ¥')}",
                        'metadata': {
                            'chapter_id': chapter_id,
                            'chapter_number': chapter_number,
                            'importance_score': plot_point.get('importance', 0.5),
                            'tags': ['æƒ…èŠ‚ç‚¹', plot_point.get('type', 'æœªçŸ¥')],
                            'is_foreshadow': 0,
                            'keyword': keyword,
                            'text_position': position,
                            'text_length': length
                        }
                    })
            
            # 4. æå–è§’è‰²çŠ¶æ€å˜åŒ–
            for i, char_state in enumerate(analysis.get('character_states', [])):
                char_name = char_state.get('character_name', 'æœªçŸ¥è§’è‰²')
                memories.append({
                    'type': 'character_event',
                    'content': f"{char_name}çš„çŠ¶æ€å˜åŒ–: {char_state.get('state_before', '')} â†’ {char_state.get('state_after', '')}ã€‚{char_state.get('psychological_change', '')}",
                    'title': f"{char_name}çš„å˜åŒ–",
                    'metadata': {
                        'chapter_id': chapter_id,
                        'chapter_number': chapter_number,
                        'importance_score': 0.7,
                        'tags': ['è§’è‰²', char_name, 'çŠ¶æ€å˜åŒ–'],
                        'related_characters': [char_name],
                        'is_foreshadow': 0
                    }
                })
            
            # 5. å¦‚æœæœ‰é‡è¦å†²çª,ä¹Ÿè®°å½•ä¸‹æ¥
            conflict = analysis.get('conflict', {})
            
            if conflict and conflict.get('level', 0) >= 7:
                # ç¡®ä¿ parties å’Œ types éƒ½æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨
                parties = conflict.get('parties', [])
                if parties and isinstance(parties, list):
                    parties = [str(p) for p in parties]
                
                types = conflict.get('types', [])
                if types and isinstance(types, list):
                    types = [str(t) for t in types]
                
                memories.append({
                    'type': 'plot_point',
                    'content': f"é‡è¦å†²çª: {conflict.get('description', '')}ã€‚å†²çªå„æ–¹: {', '.join(parties)}",
                    'title': f"å†²çª - å¼ºåº¦{conflict.get('level', 0)}",
                    'metadata': {
                        'chapter_id': chapter_id,
                        'chapter_number': chapter_number,
                        'importance_score': min(conflict.get('level', 5) / 10, 1.0),
                        'tags': ['å†²çª'] + types,
                        'is_foreshadow': 0
                    }
                })
            
            logger.info(f"ğŸ“ ä»åˆ†æä¸­æå–äº†{len(memories)}æ¡è®°å¿†")
            return memories
            
        except Exception as e:
            logger.error(f"âŒ æå–è®°å¿†å¤±è´¥: {str(e)}")
            return []
    
    def _find_text_position(self, full_text: str, keyword: str) -> tuple[int, int]:
        """
        åœ¨å…¨æ–‡ä¸­æŸ¥æ‰¾å…³é”®è¯ä½ç½®
        
        Args:
            full_text: å®Œæ•´æ–‡æœ¬
            keyword: å…³é”®è¯
        
        Returns:
            (èµ·å§‹ä½ç½®, é•¿åº¦) å¦‚æœæœªæ‰¾åˆ°è¿”å›(-1, 0)
        """
        if not keyword or not full_text:
            return (-1, 0)
        
        try:
            # 1. ç²¾ç¡®åŒ¹é…
            pos = full_text.find(keyword)
            if pos != -1:
                return (pos, len(keyword))
            
            # 2. å»é™¤æ ‡ç‚¹ç¬¦å·ååŒ¹é…
            import re
            clean_keyword = re.sub(r'[ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š""''ï¼ˆï¼‰ã€Šã€‹ã€ã€‘]', '', keyword)
            clean_text = re.sub(r'[ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š""''ï¼ˆï¼‰ã€Šã€‹ã€ã€‘]', '', full_text)
            pos = clean_text.find(clean_keyword)
            
            if pos != -1:
                # åå‘æ˜ å°„åˆ°åŸæ–‡ä½ç½®ï¼ˆç®€åŒ–å¤„ç†ï¼‰
                return (pos, len(clean_keyword))
            
            # 3. æ¨¡ç³ŠåŒ¹é…ï¼šæŸ¥æ‰¾å…³é”®è¯çš„å‰åŠéƒ¨åˆ†
            if len(keyword) > 10:
                partial = keyword[:min(15, len(keyword))]
                pos = full_text.find(partial)
                if pos != -1:
                    return (pos, len(partial))
            
            # 4. æœªæ‰¾åˆ°
            logger.debug(f"æœªæ‰¾åˆ°å…³é”®è¯ä½ç½®: {keyword[:30]}...")
            return (-1, 0)
            
        except Exception as e:
            logger.error(f"æŸ¥æ‰¾ä½ç½®å¤±è´¥: {str(e)}")
            return (-1, 0)
    
    def generate_analysis_summary(self, analysis: Dict[str, Any]) -> str:
        """
        ç”Ÿæˆåˆ†ææ‘˜è¦æ–‡æœ¬
        
        Args:
            analysis: åˆ†æç»“æœ
        
        Returns:
            æ ¼å¼åŒ–çš„æ‘˜è¦æ–‡æœ¬
        """
        try:
            lines = ["=== ç« èŠ‚åˆ†ææŠ¥å‘Š ===\n"]
            
            # æ•´ä½“è¯„åˆ†
            scores = analysis.get('scores', {})
            lines.append(f"ã€æ•´ä½“è¯„åˆ†ã€‘")
            lines.append(f"  æ•´ä½“è´¨é‡: {scores.get('overall', 'N/A')}/10")
            lines.append(f"  èŠ‚å¥æŠŠæ§: {scores.get('pacing', 'N/A')}/10")
            lines.append(f"  å¸å¼•åŠ›: {scores.get('engagement', 'N/A')}/10")
            lines.append(f"  è¿è´¯æ€§: {scores.get('coherence', 'N/A')}/10\n")
            
            # å‰§æƒ…é˜¶æ®µ
            lines.append(f"ã€å‰§æƒ…é˜¶æ®µã€‘{analysis.get('plot_stage', 'æœªçŸ¥')}\n")
            
            # é’©å­ç»Ÿè®¡
            hooks = analysis.get('hooks', [])
            if hooks:
                lines.append(f"ã€é’©å­åˆ†æã€‘å…±{len(hooks)}ä¸ª")
                for hook in hooks[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                    lines.append(f"  â€¢ [{hook.get('type')}] {hook.get('content', '')[:50]}... (å¼ºåº¦:{hook.get('strength', 0)})")
                lines.append("")
            
            # ä¼ç¬”ç»Ÿè®¡
            foreshadows = analysis.get('foreshadows', [])
            if foreshadows:
                planted = sum(1 for f in foreshadows if f.get('type') == 'planted')
                resolved = sum(1 for f in foreshadows if f.get('type') == 'resolved')
                lines.append(f"ã€ä¼ç¬”åˆ†æã€‘åŸ‹ä¸‹{planted}ä¸ª, å›æ”¶{resolved}ä¸ª\n")
            
            # å†²çªåˆ†æ
            conflict = analysis.get('conflict', {})
            if conflict:
                lines.append(f"ã€å†²çªåˆ†æã€‘")
                lines.append(f"  ç±»å‹: {', '.join(conflict.get('types', []))}")
                lines.append(f"  å¼ºåº¦: {conflict.get('level', 0)}/10")
                lines.append(f"  è¿›åº¦: {int(conflict.get('resolution_progress', 0) * 100)}%\n")
            
            # æ”¹è¿›å»ºè®®
            suggestions = analysis.get('suggestions', [])
            if suggestions:
                lines.append(f"ã€æ”¹è¿›å»ºè®®ã€‘")
                for i, sug in enumerate(suggestions, 1):
                    lines.append(f"  {i}. {sug}")
            
            return "\n".join(lines)
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆæ‘˜è¦å¤±è´¥: {str(e)}")
            return "åˆ†ææ‘˜è¦ç”Ÿæˆå¤±è´¥"


# åˆ›å»ºå…¨å±€å®ä¾‹(éœ€è¦æ—¶æ‰‹åŠ¨åˆå§‹åŒ–)
_plot_analyzer_instance = None

def get_plot_analyzer(ai_service: AIService) -> PlotAnalyzer:
    """è·å–å‰§æƒ…åˆ†æå™¨å®ä¾‹"""
    global _plot_analyzer_instance
    if _plot_analyzer_instance is None:
        _plot_analyzer_instance = PlotAnalyzer(ai_service)
    return _plot_analyzer_instance